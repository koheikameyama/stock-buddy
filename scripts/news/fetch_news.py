#!/usr/bin/env python3
"""
æ ªå¼é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã¦MarketNewsãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ©Ÿèƒ½:
- Google News RSSã‹ã‚‰æ ªå¼é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
- ã‚»ã‚¯ã‚¿ãƒ¼ãƒ»ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ + AIï¼‰
- MarketNewsãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ä¿å­˜
- è©±é¡Œã®éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰æŠ½å‡º

Usage:
    python scripts/news/fetch_news.py
"""

import sys
import re
import os
import json
import feedparser
import psycopg2
import psycopg2.extras
from typing import List, Dict, Set, Optional, Tuple
from datetime import datetime, timedelta
from openai import OpenAI

# ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹URL
RSS_URLS = {
    # Google Newsï¼ˆç·åˆï¼‰
    "google_news_stock": "https://news.google.com/rss/search?q=æ—¥æœ¬æ ª+OR+æ±è¨¼+OR+æ ªå¼å¸‚å ´+when:7d&hl=ja&gl=JP&ceid=JP:ja",
    "google_news_nikkei": "https://news.google.com/rss/search?q=site:nikkei.com+æ ª+OR+éŠ˜æŸ„+when:7d&hl=ja&gl=JP&ceid=JP:ja",
    # Google Newsï¼ˆæ±ºç®—ãƒ»æ¥­ç¸¾ï¼‰
    "google_news_earnings": "https://news.google.com/rss/search?q=æ±ºç®—+OR+æ¥­ç¸¾+OR+å¢—ç›Š+OR+æ¸›ç›Š+æ ª+when:7d&hl=ja&gl=JP&ceid=JP:ja",
    # Google Newsï¼ˆã‚»ã‚¯ã‚¿ãƒ¼åˆ¥ï¼‰
    "google_news_tech": "https://news.google.com/rss/search?q=åŠå°ä½“+OR+AIé–¢é€£+OR+ãƒ†ãƒƒã‚¯æ ª+when:7d&hl=ja&gl=JP&ceid=JP:ja",
    "google_news_auto": "https://news.google.com/rss/search?q=è‡ªå‹•è»Š+OR+EV+æ ª+when:7d&hl=ja&gl=JP&ceid=JP:ja",
    # Yahoo Finance Japan
    "yahoo_finance_market": "https://news.yahoo.co.jp/rss/topics/business.xml",
    "yahoo_finance_stock": "https://finance.yahoo.co.jp/rss/stock/domestic",
    # Bloomberg Japan
    "google_news_bloomberg": "https://news.google.com/rss/search?q=site:bloomberg.co.jp+æ ª+OR+å¸‚å ´+when:7d&hl=ja&gl=JP&ceid=JP:ja",
    # Reuters Japan
    "google_news_reuters": "https://news.google.com/rss/search?q=site:jp.reuters.com+æ ª+OR+å¸‚å ´+when:7d&hl=ja&gl=JP&ceid=JP:ja",
    # æ ªæ¢ï¼ˆKabutanï¼‰
    "google_news_kabutan": "https://news.google.com/rss/search?q=site:kabutan.jp+when:7d&hl=ja&gl=JP&ceid=JP:ja",
    # ã¿ã‚“ã‹ã¶ï¼ˆMinkabuï¼‰
    "google_news_minkabu": "https://news.google.com/rss/search?q=site:minkabu.jp+æ ª+when:7d&hl=ja&gl=JP&ceid=JP:ja",
    # æ±æ´‹çµŒæ¸ˆ
    "google_news_toyokeizai": "https://news.google.com/rss/search?q=site:toyokeizai.net+æ ª+OR+ä¼æ¥­+when:7d&hl=ja&gl=JP&ceid=JP:ja",
}

# ãƒ•ã‚£ãƒ¼ãƒ‰ã”ã¨ã®ã‚½ãƒ¼ã‚¹åãƒãƒƒãƒ”ãƒ³ã‚°
FEED_SOURCE_MAP = {
    "google_news_stock": "google_news",
    "google_news_nikkei": "nikkei",
    "google_news_earnings": "google_news",
    "google_news_tech": "google_news",
    "google_news_auto": "google_news",
    "yahoo_finance_market": "yahoo_finance",
    "yahoo_finance_stock": "yahoo_finance",
    "google_news_bloomberg": "bloomberg",
    "google_news_reuters": "reuters",
    "google_news_kabutan": "kabutan",
    "google_news_minkabu": "minkabu",
    "google_news_toyokeizai": "toyokeizai",
}

# ã‚»ã‚¯ã‚¿ãƒ¼åˆ†é¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
SECTOR_KEYWORDS = {
    "åŠå°ä½“ãƒ»é›»å­éƒ¨å“": ["åŠå°ä½“", "é›»å­éƒ¨å“", "ãƒãƒƒãƒ—", "DRAM", "NAND", "ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒª"],
    "è‡ªå‹•è»Š": ["è‡ªå‹•è»Š", "ãƒˆãƒ¨ã‚¿", "ãƒ›ãƒ³ãƒ€", "æ—¥ç”£", "ãƒãƒ„ãƒ€", "ã‚¹ãƒãƒ«", "EV", "é›»æ°—è‡ªå‹•è»Š"],
    "é‡‘è": ["éŠ€è¡Œ", "è¨¼åˆ¸", "ä¿é™º", "é‡‘è", "ãƒ¡ã‚¬ãƒãƒ³ã‚¯", "åœ°éŠ€", "ä¿¡è¨—"],
    "åŒ»è–¬å“": ["è£½è–¬", "åŒ»è–¬å“", "æ–°è–¬", "æ²»é¨“", "ãƒã‚¤ã‚ª", "å‰µè–¬"],
    "é€šä¿¡": ["é€šä¿¡", "NTT", "KDDI", "ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯", "5G", "æºå¸¯"],
    "å°å£²": ["å°å£²", "ç™¾è²¨åº—", "ã‚³ãƒ³ãƒ“ãƒ‹", "EC", "é€šè²©", "ã‚¹ãƒ¼ãƒ‘ãƒ¼"],
    "ä¸å‹•ç”£": ["ä¸å‹•ç”£", "ãƒãƒ³ã‚·ãƒ§ãƒ³", "ã‚ªãƒ•ã‚£ã‚¹", "REIT", "å•†æ¥­æ–½è¨­"],
    "ã‚¨ãƒãƒ«ã‚®ãƒ¼": ["çŸ³æ²¹", "ã‚¬ã‚¹", "é›»åŠ›", "ã‚¨ãƒãƒ«ã‚®ãƒ¼", "å†ç”Ÿå¯èƒ½", "å¤ªé™½å…‰"],
    "ç´ æ": ["é‰„é‹¼", "åŒ–å­¦", "ç´ æ", "å»ºæ", "ã‚»ãƒ¡ãƒ³ãƒˆ"],
    "ITãƒ»ã‚µãƒ¼ãƒ“ã‚¹": ["IT", "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢", "ã‚¯ãƒ©ã‚¦ãƒ‰", "AI", "DX", "SaaS"],
}

# ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†é¡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
SENTIMENT_KEYWORDS = {
    "positive": ["æ€¥é¨°", "ä¸Šæ˜‡", "å¥½èª¿", "æœ€é«˜ç›Š", "å¢—ç›Š", "è²·ã„", "å¼·æ°—", "ä¸Šæ–¹ä¿®æ­£", "å¥½æ±ºç®—"],
    "negative": ["æ€¥è½", "ä¸‹è½", "æ¸›ç›Š", "èµ¤å­—", "å£²ã‚Š", "å¼±æ°—", "æ‡¸å¿µ", "ä¸‹æ–¹ä¿®æ­£", "ä¸èª¿"],
    "neutral": ["æ¨ªã°ã„", "æ§˜å­è¦‹", "ä¿ã¡åˆã„", "å¤‰ã‚ã‚‰ãš", "æ®ãˆç½®ã"],
}


def fetch_rss_feed(url: str) -> List[Dict]:
    """
    RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚¹ã™ã‚‹

    Args:
        url: RSS URL

    Returns:
        ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆ
    """
    try:
        print(f"ğŸ“¡ Fetching RSS from {url}")
        feed = feedparser.parse(url)

        if feed.bozo:
            print(f"âš ï¸  Warning: Feed parsing error: {feed.bozo_exception}")

        entries = []
        for entry in feed.entries:
            entries.append({
                "title": entry.get("title", ""),
                "link": entry.get("link", ""),
                "summary": entry.get("summary", ""),
                "published": entry.get("published", ""),
                "published_parsed": entry.get("published_parsed", None),
            })

        print(f"âœ… Fetched {len(entries)} entries")
        return entries

    except Exception as e:
        print(f"âŒ Error fetching RSS: {e}")
        return []


def extract_stock_codes(text: str) -> Set[str]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ï¼ˆ4æ¡æ•°å­—ï¼‰ã‚’æŠ½å‡º

    Args:
        text: æ¤œç´¢å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        æŠ½å‡ºã•ã‚ŒãŸéŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã®ã‚»ãƒƒãƒˆ
    """
    # 4æ¡ã®æ•°å­—ã‚’æŠ½å‡ºï¼ˆãŸã ã—1000-9999ã®ç¯„å›²ï¼‰
    pattern = r'\b([1-9][0-9]{3})\b'
    matches = re.findall(pattern, text)

    # éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã¨ã—ã¦å¦¥å½“ãªç¯„å›²ï¼ˆ1000-9999ï¼‰ã®ã¿ã‚’è¿”ã™
    stock_codes = {code for code in matches if 1000 <= int(code) <= 9999}

    return stock_codes


def filter_recent_entries(entries: List[Dict], days: int = 7) -> List[Dict]:
    """
    ç›´è¿‘Næ—¥é–“ã®ã‚¨ãƒ³ãƒˆãƒªã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

    Args:
        entries: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆ
        days: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹æ—¥æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 7æ—¥ï¼‰

    Returns:
        ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆ
    """
    cutoff_date = datetime.now() - timedelta(days=days)

    recent_entries = []
    for entry in entries:
        if entry["published_parsed"]:
            entry_date = datetime(*entry["published_parsed"][:6])
            if entry_date >= cutoff_date:
                recent_entries.append(entry)

    return recent_entries


def detect_sector_by_keywords(text: str) -> Optional[str]:
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã§ã‚»ã‚¯ã‚¿ãƒ¼ã‚’åˆ¤å®š

    Args:
        text: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ« + å†…å®¹

    Returns:
        ã‚»ã‚¯ã‚¿ãƒ¼åï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯Noneï¼‰
    """
    text_lower = text.lower()

    for sector, keywords in SECTOR_KEYWORDS.items():
        for keyword in keywords:
            if keyword.lower() in text_lower:
                return sector

    return None


def detect_sentiment_by_keywords(text: str) -> Optional[str]:
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã§ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚’åˆ¤å®š

    Args:
        text: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ« + å†…å®¹

    Returns:
        ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆï¼ˆpositive/neutral/negativeï¼‰ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯Noneï¼‰
    """
    text_lower = text.lower()

    for sentiment, keywords in SENTIMENT_KEYWORDS.items():
        for keyword in keywords:
            if keyword.lower() in text_lower:
                return sentiment

    return None


def analyze_with_openai(title: str, content: str) -> Tuple[Optional[str], Optional[str]]:
    """
    OpenAI APIã§ã‚»ã‚¯ã‚¿ãƒ¼ãƒ»ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚’åˆ†æ

    Args:
        title: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«
        content: ãƒ‹ãƒ¥ãƒ¼ã‚¹å†…å®¹

    Returns:
        (sector, sentiment) ã®ã‚¿ãƒ—ãƒ«
    """
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âš ï¸  OPENAI_API_KEY not found, skipping AI analysis")
            return None, None

        client = OpenAI(api_key=api_key)

        prompt = f"""ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åˆ†æã—ã¦ã€ã‚»ã‚¯ã‚¿ãƒ¼ã¨ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {title}
å†…å®¹: {content}

ã‚»ã‚¯ã‚¿ãƒ¼å€™è£œ: åŠå°ä½“ãƒ»é›»å­éƒ¨å“ã€è‡ªå‹•è»Šã€é‡‘èã€åŒ»è–¬å“ã€é€šä¿¡ã€å°å£²ã€ä¸å‹•ç”£ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼ã€ç´ æã€ITãƒ»ã‚µãƒ¼ãƒ“ã‚¹
ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆå€™è£œ: positiveã€neutralã€negative

å›ç­”å½¢å¼ï¼ˆJSONï¼‰:
{{"sector": "ã‚»ã‚¯ã‚¿ãƒ¼å or null", "sentiment": "positive/neutral/negative or null"}}
"""

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.3,
        )

        result = json.loads(response.choices[0].message.content)
        sector = result.get("sector")
        sentiment = result.get("sentiment")

        return sector, sentiment

    except Exception as e:
        print(f"âš ï¸  OpenAI API error: {e}")
        return None, None


def check_duplicate_news(title: str, url: str, cursor) -> bool:
    """
    é‡è¤‡ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¿ã‚¤ãƒˆãƒ« + URLã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åˆ¤å®šï¼‰

    Args:
        title: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«
        url: ãƒ‹ãƒ¥ãƒ¼ã‚¹URL
        cursor: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚«ãƒ¼ã‚½ãƒ«

    Returns:
        True: é‡è¤‡ã‚ã‚Šã€False: é‡è¤‡ãªã—
    """
    try:
        cursor.execute(
            """
            SELECT COUNT(*) FROM "MarketNews"
            WHERE title = %s AND url = %s
            """,
            (title, url)
        )
        count = cursor.fetchone()[0]
        return count > 0
    except Exception as e:
        print(f"âš ï¸  Error checking duplicate: {e}")
        return False


def save_news_to_db(news_items: List[Dict], conn) -> int:
    """
    MarketNewsãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ä¿å­˜ï¼ˆãƒãƒƒãƒINSERTï¼‰

    Args:
        news_items: ä¿å­˜ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒªã‚¹ãƒˆ
        conn: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š

    Returns:
        ä¿å­˜ã—ãŸä»¶æ•°
    """
    if not news_items:
        return 0

    try:
        cursor = conn.cursor()

        # ãƒãƒƒãƒINSERTç”¨ã®ãƒ‡ãƒ¼ã‚¿æº–å‚™
        values = [
            (
                item["title"],
                item["content"],
                item["url"],
                item["source"],
                item["sector"],
                item["sentiment"],
                item["published_at"],
            )
            for item in news_items
        ]

        # cuidã®ç”Ÿæˆï¼ˆPrismaã®@default(cuid())ã‚’æ¨¡å€£ï¼‰
        import secrets
        import base64

        def generate_cuid():
            """Prismaã®cuidã‚’æ¨¡å€£ã—ãŸIDç”Ÿæˆ"""
            random_bytes = secrets.token_bytes(12)
            return base64.urlsafe_b64encode(random_bytes).decode('utf-8').rstrip('=')

        values_with_id = [
            (
                generate_cuid(),
                item["title"],
                item["content"],
                item["url"],
                item["source"],
                item["sector"],
                item["sentiment"],
                item["published_at"],
            )
            for item in news_items
        ]

        psycopg2.extras.execute_values(
            cursor,
            """
            INSERT INTO "MarketNews"
            (id, title, content, url, source, sector, sentiment, "publishedAt", "createdAt")
            VALUES %s
            """,
            values_with_id,
            template='(%s, %s, %s, %s, %s, %s, %s, %s, NOW())',
            page_size=100
        )

        conn.commit()
        cursor.close()

        return len(news_items)

    except Exception as e:
        print(f"âŒ Error saving news to DB: {e}")
        conn.rollback()
        return 0


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("JPX News & Stock Code Extraction Script (Enhanced)")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    database_url = os.getenv("DATABASE_URL")
    conn = None

    if database_url:
        try:
            print(f"\nğŸ”Œ Connecting to database...")
            conn = psycopg2.connect(database_url)
            print("âœ… Database connected")
        except Exception as e:
            print(f"âŒ Database connection failed: {e}")
            print("âš ï¸  Continuing without database (stock code extraction only)")
    else:
        print("âš ï¸  DATABASE_URL not found, skipping database operations")

    all_stock_codes = set()
    all_entries = []
    news_to_save = []

    rule_based_count = 0
    ai_based_count = 0

    # å„RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—
    for feed_name, url in RSS_URLS.items():
        print(f"\nğŸ“° Processing feed: {feed_name}")
        entries = fetch_rss_feed(url)

        # ç›´è¿‘7æ—¥é–“ã®ã‚¨ãƒ³ãƒˆãƒªã®ã¿ã‚’å¯¾è±¡
        recent_entries = filter_recent_entries(entries, days=7)
        print(f"â„¹ï¸  Recent entries (last 7 days): {len(recent_entries)}")

        # å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å‡¦ç†
        for entry in recent_entries:
            text = f"{entry['title']} {entry['summary']}"

            # éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆæ—¢å­˜æ©Ÿèƒ½ï¼‰
            stock_codes = extract_stock_codes(text)
            if stock_codes:
                entry["stock_codes"] = list(stock_codes)
                all_stock_codes.update(stock_codes)
                all_entries.append({
                    "feed": feed_name,
                    "title": entry["title"],
                    "link": entry["link"],
                    "stock_codes": list(stock_codes),
                    "published": entry["published"],
                })

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ç”¨ã®å‡¦ç†
            if conn:
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                cursor = conn.cursor()
                if check_duplicate_news(entry["title"], entry["link"], cursor):
                    cursor.close()
                    continue
                cursor.close()

                # ã‚»ã‚¯ã‚¿ãƒ¼ãƒ»ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
                sector = detect_sector_by_keywords(text)
                sentiment = detect_sentiment_by_keywords(text)

                # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§åˆ¤å®šã§ããªã‹ã£ãŸå ´åˆã¯AIåˆ†æ
                if sector is None or sentiment is None:
                    ai_sector, ai_sentiment = analyze_with_openai(entry["title"], entry["summary"])
                    if sector is None:
                        sector = ai_sector
                    if sentiment is None:
                        sentiment = ai_sentiment
                    ai_based_count += 1
                else:
                    rule_based_count += 1

                # ä¿å­˜ç”¨ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
                published_at = datetime(*entry["published_parsed"][:6]) if entry["published_parsed"] else datetime.now()
                source_name = FEED_SOURCE_MAP.get(feed_name, "google_news")
                news_to_save.append({
                    "title": entry["title"],
                    "content": entry["summary"],
                    "url": entry["link"],
                    "source": source_name,
                    "sector": sector,
                    "sentiment": sentiment,
                    "published_at": published_at,
                })

    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
    if conn and news_to_save:
        print(f"\nğŸ” Analyzing {len(news_to_save)} new entries...")
        print(f"  â”œâ”€ Rule-based: {rule_based_count} entries")
        print(f"  â””â”€ AI-based: {ai_based_count} entries")

        saved_count = save_news_to_db(news_to_save, conn)
        print(f"ğŸ’¾ Saved {saved_count} news to database")

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’ã‚¯ãƒ­ãƒ¼ã‚º
    if conn:
        conn.close()
        print("ğŸ”Œ Database connection closed")

    # çµæœã‚’è¡¨ç¤º
    print(f"\n{'=' * 60}")
    print(f"ğŸ“Š Summary")
    print(f"{'=' * 60}")
    print(f"Total unique stock codes found: {len(all_stock_codes)}")
    print(f"Stock codes: {sorted(all_stock_codes)}")

    print(f"\nğŸ“‹ News entries with stock codes:")
    for entry in all_entries:
        print(f"\n  â€¢ {entry['title']}")
        print(f"    Codes: {entry['stock_codes']}")
        print(f"    Link: {entry['link']}")
        print(f"    Date: {entry['published']}")

    # éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã‚’JSONå½¢å¼ã§å‡ºåŠ›ï¼ˆæ¬¡ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ä½¿ç”¨ï¼‰
    output_file = "scripts/news/trending_stock_codes.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump({
            "timestamp": datetime.now().isoformat(),
            "stock_codes": sorted(all_stock_codes),
            "news_count": len(all_entries),
        }, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… Stock codes saved to {output_file}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
