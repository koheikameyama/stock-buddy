#!/usr/bin/env python3
"""
JPXå…¬å¼RSSã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã€è©±é¡Œã®éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Usage:
    python scripts/news/fetch_jpx_news.py
"""

import sys
import re
import feedparser
from typing import List, Dict, Set
from datetime import datetime, timedelta

# ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹URLï¼ˆGoogle News RSSã‚’ä½¿ç”¨ï¼‰
RSS_URLS = {
    "google_news_stock": "https://news.google.com/rss/search?q=æ—¥æœ¬æ ª+OR+æ±è¨¼+OR+æ ªå¼å¸‚å ´+when:7d&hl=ja&gl=JP&ceid=JP:ja",
    "google_news_nikkei": "https://news.google.com/rss/search?q=site:nikkei.com+æ ª+OR+éŠ˜æŸ„+when:7d&hl=ja&gl=JP&ceid=JP:ja",
}


def fetch_rss_feed(url: str) -> List[Dict]:
    """
    RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚¹ã™ã‚‹

    Args:
        url: RSS URL

    Returns:
        ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆ
    """
    try:
        print(f"ğŸ“¡ Fetching RSS from {url}")
        feed = feedparser.parse(url)

        if feed.bozo:
            print(f"âš ï¸  Warning: Feed parsing error: {feed.bozo_exception}")

        entries = []
        for entry in feed.entries:
            entries.append({
                "title": entry.get("title", ""),
                "link": entry.get("link", ""),
                "summary": entry.get("summary", ""),
                "published": entry.get("published", ""),
                "published_parsed": entry.get("published_parsed", None),
            })

        print(f"âœ… Fetched {len(entries)} entries")
        return entries

    except Exception as e:
        print(f"âŒ Error fetching RSS: {e}")
        return []


def extract_stock_codes(text: str) -> Set[str]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ï¼ˆ4æ¡æ•°å­—ï¼‰ã‚’æŠ½å‡º

    Args:
        text: æ¤œç´¢å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        æŠ½å‡ºã•ã‚ŒãŸéŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã®ã‚»ãƒƒãƒˆ
    """
    # 4æ¡ã®æ•°å­—ã‚’æŠ½å‡ºï¼ˆãŸã ã—1000-9999ã®ç¯„å›²ï¼‰
    pattern = r'\b([1-9][0-9]{3})\b'
    matches = re.findall(pattern, text)

    # éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã¨ã—ã¦å¦¥å½“ãªç¯„å›²ï¼ˆ1000-9999ï¼‰ã®ã¿ã‚’è¿”ã™
    stock_codes = {code for code in matches if 1000 <= int(code) <= 9999}

    return stock_codes


def filter_recent_entries(entries: List[Dict], days: int = 7) -> List[Dict]:
    """
    ç›´è¿‘Næ—¥é–“ã®ã‚¨ãƒ³ãƒˆãƒªã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

    Args:
        entries: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆ
        days: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹æ—¥æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 7æ—¥ï¼‰

    Returns:
        ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆ
    """
    cutoff_date = datetime.now() - timedelta(days=days)

    recent_entries = []
    for entry in entries:
        if entry["published_parsed"]:
            entry_date = datetime(*entry["published_parsed"][:6])
            if entry_date >= cutoff_date:
                recent_entries.append(entry)

    return recent_entries


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("JPX News & Stock Code Extraction Script")
    print("=" * 60)

    all_stock_codes = set()
    all_entries = []

    # å„RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—
    for feed_name, url in RSS_URLS.items():
        print(f"\nğŸ“° Processing feed: {feed_name}")
        entries = fetch_rss_feed(url)

        # ç›´è¿‘7æ—¥é–“ã®ã‚¨ãƒ³ãƒˆãƒªã®ã¿ã‚’å¯¾è±¡
        recent_entries = filter_recent_entries(entries, days=7)
        print(f"â„¹ï¸  Recent entries (last 7 days): {len(recent_entries)}")

        # éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        for entry in recent_entries:
            text = f"{entry['title']} {entry['summary']}"
            stock_codes = extract_stock_codes(text)

            if stock_codes:
                entry["stock_codes"] = list(stock_codes)
                all_stock_codes.update(stock_codes)
                all_entries.append({
                    "feed": feed_name,
                    "title": entry["title"],
                    "link": entry["link"],
                    "stock_codes": list(stock_codes),
                    "published": entry["published"],
                })

    # çµæœã‚’è¡¨ç¤º
    print(f"\n{'=' * 60}")
    print(f"ğŸ“Š Summary")
    print(f"{'=' * 60}")
    print(f"Total unique stock codes found: {len(all_stock_codes)}")
    print(f"Stock codes: {sorted(all_stock_codes)}")

    print(f"\nğŸ“‹ News entries with stock codes:")
    for entry in all_entries:
        print(f"\n  â€¢ {entry['title']}")
        print(f"    Codes: {entry['stock_codes']}")
        print(f"    Link: {entry['link']}")
        print(f"    Date: {entry['published']}")

    # éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã‚’JSONå½¢å¼ã§å‡ºåŠ›ï¼ˆæ¬¡ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ä½¿ç”¨ï¼‰
    import json
    output_file = "scripts/news/trending_stock_codes.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump({
            "timestamp": datetime.now().isoformat(),
            "stock_codes": sorted(all_stock_codes),
            "news_count": len(all_entries),
        }, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… Stock codes saved to {output_file}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
